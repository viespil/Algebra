{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viespil/Algebra/blob/master/ALC_1C2025_TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 1\n",
        "### a)\n",
        "\n",
        "Para que un vector $ v $ sea autovector de una matriz $M$, debe cumplirse la siguiente igualdad:\n",
        "\n",
        "$$\n",
        "M \\cdot v = \\lambda \\cdot v\n",
        "$$\n",
        "\n",
        "Donde $ \\lambda $ es un escalar llamado autovalor.\n",
        "\n",
        "En este caso, queremos analizar si el vector de unos $ \\mathbf{1} = (1, 1, \\dots, 1)^T $ es autovector de la matriz $ M $. Para eso, multiplicamos:\n",
        "\n",
        "$$\n",
        "M \\cdot \\mathbf{1} = \\lambda \\cdot \\mathbf{1}\n",
        "$$\n",
        "\n",
        "Luego el vector de unos será autovector de $ M $ si y solo si $M \\cdot \\mathbf{1}$ da como resultado un vector múltiplo del vector de unos.\n",
        "\n",
        "Multiplicar una matriz por el vector de unos equivale a sumar las filas de la matriz. Es decir:\n",
        "\n",
        "$$\n",
        "(M \\cdot \\mathbf{1})_i = \\sum_{k=1}^{n} M_{ik}\n",
        "$$\n",
        "\n",
        "Ahora apliquemos esto a $ L = K - A $:\n",
        "\n",
        "- La matriz $ K $ es la matriz diagonal de grados, es decir:\n",
        "\n",
        "$$\n",
        "K_{ij} = \\begin{cases}\n",
        "k_i & \\text{si } i = j \\\\\n",
        "0 & \\text{si } i \\ne j\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "donde $ k_i $ representa el grado del nodo $ i $ (cantidad de conexiones que tiene el nodo $ i $).\n",
        "\n",
        "Cuando multiplicamos $ K $ por $ \\mathbf{1} $, cada componente $ i $-ésima del resultado es:\n",
        "\n",
        "$$\n",
        "(K \\cdot \\mathbf{1})_i = \\sum_{k=1}^n K_{ik} \\cdot 1 = K_{ii}\n",
        "$$\n",
        "\n",
        "Ya que todos los elementos fuera de la diagonal son cero, y en la diagonal $ K_{ii} = k_i $, entonces:\n",
        "\n",
        "$$\n",
        "(K \\cdot \\mathbf{1})_i = k_i\n",
        "$$\n",
        "\n",
        "El resultado es un vector con los grados de cada nodo.\n",
        "\n",
        "$$\n",
        "K \\cdot \\mathbf{1} = \\vec{k}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- $ A $ es la matriz de adyacencia, es decir:\n",
        "\n",
        "$$\n",
        "A_{ik} = \\begin{cases}\n",
        "1 & \\text{si existe una arista entre el nodo } i \\text{ y el nodo } k \\\\\n",
        "0 & \\text{en caso contrario}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "El producto entre la matriz de adyacencia $ A $ y el vector de unos $ \\mathbf{1} $ nos da un vector donde cada componente $ i $-ésima es:\n",
        "\n",
        "$$\n",
        "(A \\cdot \\mathbf{1})_i = \\sum_{k=1}^n A_{ik}\n",
        "$$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$\n",
        "(A \\cdot \\mathbf{1})_i = \\sum_{k=1}^{n} \\left( 1 \\text{ si hay arista entre } i \\text{ y } k; \\ 0 \\text{ si no} \\right)\n",
        "$\n",
        "\n",
        "Por lo tanto, $ (A \\cdot \\mathbf{1})_i $ cuenta cuántas conexiones tiene el nodo $ i $, es decir, su grado:\n",
        "\n",
        "$$\n",
        "(A \\cdot \\mathbf{1})_i = k_i\n",
        "$$\n",
        "\n",
        "Al igual que con K:\n",
        "\n",
        "$$\n",
        "A \\cdot \\mathbf{1} = \\vec{k}\n",
        "$$\n",
        "\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "K \\cdot \\mathbf{1} = A \\cdot \\mathbf{1}\n",
        "\\Rightarrow L \\cdot \\mathbf{1} = (K - A) \\cdot \\mathbf{1} = \\mathbf{0}\n",
        "$$\n",
        "\n",
        "Esto demuestra que $ \\mathbf{1} $ es autovector de $ L $, con autovalor $ \\lambda = 0 $.\n",
        "\n",
        "Veamos que pasa con R = A - P\n",
        "\n",
        "Para ello, primero vamos a demostrar que en todo grafo no dirigido:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j = 2E\n",
        "$$\n",
        "\n",
        "donde $k_j$ es el grado del nodo $j$, y $E$ es el número total de aristas.\n",
        "\n",
        "Caso base: $E = 0$\n",
        "\n",
        "Si el grafo tiene cero aristas, entonces todos los nodos tienen grado cero, es decir $k_j = 0$ para todo $j$.\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j = 0 = 2 \\cdot 0 = 2E\n",
        "$$\n",
        "\n",
        "El caso base se cumple.\n",
        "\n",
        "Paso inductivo: supongamos que para un grafo con $E = m$ aristas se cumple que:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j = 2m\n",
        "$$\n",
        "\n",
        "Queremos demostrar que también se cumple para un grafo con $E = m+1$ aristas.\n",
        "\n",
        "Agregamos una nueva arista al grafo. Esa arista conecta dos nodos $u$ y $v$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "- El grado de $u$ aumenta en 1: $k_u \\mapsto k_u + 1$\n",
        "- El grado de $v$ también aumenta en 1: $k_v \\mapsto k_v + 1$\n",
        "\n",
        "Por lo tanto, la suma total de grados aumenta en 2:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j \\mapsto \\sum_{j=1}^n k_j + 2\n",
        "$$\n",
        "\n",
        "Como por hipótesis inductiva $\\sum_{j=1}^n k_j = 2m$, se tiene que:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j = 2m + 2 = 2(m + 1) = 2E\n",
        "$$\n",
        "\n",
        "Se cumple para $E = m+1$\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j = 2E \\quad \\text{para todo grafo no dirigido con } E \\text{ aristas.}\n",
        "$$\n",
        "\n",
        "Ahora si, queremos mostrar que el vector de unos $ \\mathbf{1} $ es autovector de $ R = A - P $.\n",
        "\n",
        "\n",
        "Recordemos que $A \\cdot \\mathbf{1} = \\vec{k}$\n",
        "\n",
        "La matriz $ P $ tiene entradas:\n",
        "\n",
        "$$\n",
        "P_{ij} = \\frac{k_i k_j}{2E}\n",
        "$$\n",
        "\n",
        "Multiplicamos $ P $ por $ \\mathbf{1} $:\n",
        "\n",
        "$$\n",
        "(P \\cdot \\mathbf{1})_i = \\sum_{j=1}^n \\frac{k_i k_j}{2E} = \\frac{k_i}{2E} \\sum_{j=1}^n k_j\n",
        "$$\n",
        "\n",
        "Y como demostramos anteriormente:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^n k_j = 2E\n",
        "$$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "(P \\cdot \\mathbf{1})_i = \\frac{k_i}{2E} \\cdot 2E = k_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "P \\cdot \\mathbf{1} = \\vec{k}\n",
        "$$\n",
        "\n",
        "Luego $A \\cdot \\mathbf{1} = P \\cdot \\mathbf{1} $\n",
        "\n",
        "\n",
        "$$\n",
        "R \\cdot \\mathbf{1} = (A - P) \\cdot \\mathbf{1} = A \\cdot \\mathbf{1} - P \\cdot \\mathbf{1} = 0\n",
        "$$\n",
        "\n",
        "Por lo tanto:\n",
        "\n",
        "$$\n",
        "R \\cdot \\mathbf{1} = 0 \\cdot \\mathbf{1}\n",
        "$$\n",
        "\n",
        "$\\mathbf{1} $ es autovector de $ R $, con autovalor 0.\n",
        "\n",
        "\n",
        "Las particiones de la red en dos comunidades se representan mediante un vector $ s \\in \\mathbb{R}^n $, donde cada componente $ s_i $ vale:\n",
        "\n",
        "- $ +1 $ si el nodo $ i $ pertenece al grupo 1,\n",
        "- $ -1 $ si pertenece al grupo 2.\n",
        "\n",
        "El vector de unos $ \\mathbf{1} $ significa que todos los nodos están asignados al mismo grupo, y por lo tanto no hay partición. Es decir, la red completa se considera una única comunidad.\n",
        "\n",
        "### b)\n",
        "\n",
        "Primero vamos a demostrar que si una matriz $ M \\in \\mathbb{R}^{n \\times n} $ es simétrica, entonces si tiene dos autovectores $v_1$ y $v_2$ asociados a autovalores distintos entonces $v_1^T v_2 = 0$.\n",
        "\n",
        "Supongamos que:\n",
        "\n",
        "- $ M $ es simétrica: $ M = M^T $,\n",
        "- $ v_1 $ es autovector de $ M $ con autovalor $ \\lambda_1 $,\n",
        "- $ v_2 $ es autovector de $ M $ con autovalor $ \\lambda_2 $,\n",
        "- y $ \\lambda_1 \\ne \\lambda_2 $.\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "M v_1 = \\lambda_1 v_1 \\\\\n",
        "M v_2 = \\lambda_2 v_2\n",
        "$$\n",
        "\n",
        "Multiplicamos ambos lados de la primera ecuación por $v_2^T$ (por izquierda):\n",
        "\n",
        "$$\n",
        "v_2^T (M v_1) = v_2^T (\\lambda_1 v_1) = \\lambda_1 (v_2^T v_1)\n",
        "\\quad \\text{(1)}\n",
        "$$\n",
        "\n",
        "Ahora usamos simetría de $ M $ para reescribir el miembro izquierdo:\n",
        "\n",
        "$$\n",
        "v_2^T (M v_1) = v_2^T (M^T v_1) = (v_2^T M^T) v_1 = (M v_2)^T v_1 = (\\lambda_2 v_2)^T v_1 = \\lambda_2 (v_2^T v_1)\n",
        "\\quad \\text{(2)}\n",
        "$$\n",
        "\n",
        "Igualando (1) y (2):\n",
        "\n",
        "$$\n",
        "\\lambda_1 (v_2^T v_1) = \\lambda_2 (v_2^T v_1)\n",
        "\\Rightarrow (\\lambda_1 - \\lambda_2)(v_2^T v_1) = 0\n",
        "$$\n",
        "\n",
        "Como $ \\lambda_1 \\ne \\lambda_2 $, se deduce que:\n",
        "\n",
        "$$\n",
        "v_1^T v_2 = 0\n",
        "$$\n",
        "\n",
        "Es decir, los autovectores $ v_1 $ y $ v_2 $ son ortogonales.\n",
        "\n",
        "Si ahora logramos demostrar que tanto L como R son simétricas habrá quedado demostrado que se cumple para ambas.\n",
        "\n",
        "Para que una matriz $M$ sea simétrica, debe cumplirse que $M_{ij} = M_{ji} \\quad \\text{para todo } i, j$\n",
        "\n",
        "$K$ es una matriz diagonal, donde:\n",
        "\n",
        "- $K_{ij} = 0$ si $i \\ne j$\n",
        "- $K_{ii} = k_i$\n",
        "\n",
        "Entonces, para cada entrada $K_{ij}$:\n",
        "\n",
        "- Si $i \\ne j$: $K_{ij} = K_{ji} = 0$\n",
        "- Si $i = j$: $K_{ii} = K_{ii}$\n",
        "\n",
        "Por lo tanto, se cumple que $K_{ij} = K_{ji}$ para todo $i, j$  \n",
        "$\\Rightarrow$ $K$ es simétrica\n",
        "\n",
        "$A$ es la matriz que representa conexiones entre nodos.\n",
        "\n",
        "- $A_{ij} = 1$ si hay arista entre $i$ y $j$\n",
        "- $A_{ij} = 0$ si no hay arista\n",
        "\n",
        "Como el grafo es no dirigido, si hay arista entre $i$ y $j$ entonces también hay arista entre $j$ y $i$:\n",
        "\n",
        "$$\n",
        "A_{ij} = A_{ji}\n",
        "$$\n",
        "\n",
        "Por lo tanto, se cumple que $A_{ij} = A_{ji}$ para todo $i, j$  \n",
        "$\\Rightarrow$ $A$ es simétrica\n",
        "\n",
        "\n",
        "Sea $L_{ij} = K_{ij} - A_{ij}$. Como $K$ y $A$ son simétricas, se cumple:\n",
        "\n",
        "$$\n",
        "L_{ij} = K_{ij} - A_{ij} \\\\\n",
        "L_{ji} = K_{ji} - A_{ji}\n",
        "$$\n",
        "\n",
        "Pero como $K_{ij} = K_{ji}$ y $A_{ij} = A_{ji}$, entonces:\n",
        "\n",
        "$$\n",
        "L_{ij} = L_{ji}\n",
        "$$\n",
        "\n",
        "$\\Rightarrow$ $L$ es simétrica\n",
        "\n",
        "\n",
        "$P$ se define como:\n",
        "\n",
        "$$\n",
        "P_{ij} = \\frac{k_i k_j}{2E}\n",
        "$$\n",
        "\n",
        "Esto es un producto simétrico: cambiar $i$ por $j$ no modifica el valor:\n",
        "\n",
        "$$\n",
        "P_{ij} = \\frac{k_i k_j}{2E} = \\frac{k_j k_i}{2E} = P_{ji}\n",
        "$$\n",
        "\n",
        "$\\Rightarrow$ $P$ es simétrica\n",
        "\n",
        "\n",
        "Similar al caso anterior:\n",
        "\n",
        "$$\n",
        "R_{ij} = A_{ij} - P_{ij} \\\\\n",
        "R_{ji} = A_{ji} - P_{ji}\n",
        "$$\n",
        "\n",
        "Como $A$ y $P$ son simétricas, se cumple:\n",
        "\n",
        "$$\n",
        "R_{ij} = R_{ji}\n",
        "$$\n",
        "\n",
        "$\\Rightarrow$ $R$ es simétrica\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### c)\n",
        "\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n v_i = 0 \\iff v \\text{ es ortogonal al vector de unos } \\mathbf{1}\n",
        "$$\n",
        "\n",
        "Dos vectores $v$ y $w$ son ortogonales si su producto interno es cero:\n",
        "\n",
        "$$\n",
        "v^T w = 0\n",
        "$$\n",
        "\n",
        "En nuestro caso, sea $\\mathbf{1} = (1, 1, \\dots, 1)^T$, un vector de tamaño $n$. Entonces:\n",
        "\n",
        "$$\n",
        "\\mathbf{1}^T v = \\sum_{i=1}^n 1 \\cdot v_i = \\sum_{i=1}^n v_i\n",
        "$$\n",
        "\n",
        "Conclusión\n",
        "\n",
        "$$\n",
        "\\mathbf{1}^T v = 0 \\iff \\sum_{i=1}^n v_i = 0\n",
        "$$\n",
        "\n",
        "Queremos demostrar que si $v$ es autovector de $L$ o de $R$ con autovalor $\\lambda \\ne 0$, entonces $\\sum_{i=1}^n v_i = 0$ o también, como demostramos previamente, que $\\mathbf{1}^T v = 0$\n",
        "\n",
        "Para la matriz $L = K - A$:\n",
        "\n",
        "Supongamos que $L v = \\lambda v$ con $\\lambda \\ne 0$.\n",
        "\n",
        "Multiplicamos por $\\mathbf{1}^T$ por izquierda:\n",
        "\n",
        "$$\n",
        "\\mathbf{1}^T L v = \\lambda \\cdot \\mathbf{1}^T v\n",
        "$$\n",
        "\n",
        "Del punto anterior sabemos que $L \\cdot \\mathbf{1} = 0$, por lo tanto:\n",
        "\n",
        "$$\n",
        "\\mathbf{1}^T L = (L \\cdot \\mathbf{1})^T = 0^T\n",
        "\\Rightarrow \\mathbf{1}^T L v = 0\n",
        "$$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "\\lambda \\cdot \\mathbf{1}^T v = 0\n",
        "\\Rightarrow \\mathbf{1}^T v = 0\n",
        "$$\n",
        "\n",
        "Como $\\lambda \\ne 0$, concluimos que $v$ es ortogonal a $\\mathbf{1}$.\n",
        "\n",
        "Para la matriz $R = A - P$\n",
        "\n",
        "Supongamos que $R v = \\lambda v$ con $\\lambda \\ne 0$.\n",
        "\n",
        "Como vimos en el punto anterior, $R \\cdot \\mathbf{1} = 0$, entonces:\n",
        "\n",
        "$$\n",
        "\\mathbf{1}^T R v = \\lambda \\cdot \\mathbf{1}^T v = 0\n",
        "\\Rightarrow \\mathbf{1}^T v = 0\n",
        "$$\n",
        "\n",
        "Conclusión\n",
        "\n",
        "Si $v$ es autovector de $L$ o de $R$ con autovalor no nulo, entonces:\n",
        "\n",
        "$$\n",
        "v \\perp \\mathbf{1}\n",
        "$$\n",
        "\n",
        "Es decir, la suma de las componentes de $v$ es cero.\n",
        "\n"
      ],
      "metadata": {
        "id": "AlUxOsiwykyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 2\n",
        "### a)\n",
        "\n",
        "Queremos demostrar que si $M$ es una matriz diagonalizable con autovalores $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ y autovectores $v_1, v_2, \\dots, v_n$, entonces:\n",
        "\n",
        "1. Los autovalores de $M + \\mu I$ son $\\gamma_i = \\lambda_i + \\mu$  \n",
        "2. Los autovectores asociados siguen siendo $v_i$  \n",
        "3. Si $\\mu + \\lambda_i \\ne 0$ para todo $i$, entonces $M + \\mu I$ es invertible\n",
        "\n",
        "Para que un vector $ v_i $ sea autovector de una matriz $M$, debe cumplirse la siguiente igualdad:\n",
        "\n",
        "$$\n",
        "M \\cdot v_i = \\lambda_i \\cdot v_i \\quad \\text{(1)}\n",
        "$$\n",
        "\n",
        "\n",
        "Reemplazamos $M$ por $M + \\mu I$ y buscamos sus autovalores y autovectores.\n",
        "\n",
        "$$\n",
        "(M + \\mu I) \\cdot v_i = \\lambda_i \\cdot v_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "(M + \\mu I) \\cdot v_i = M\\cdot v_i + \\mu I \\cdot v_i = M\\cdot v_i + \\mu \\cdot v_i \\overset{(1)}{=} \\lambda_i \\cdot v_i + \\mu \\cdot v_i = (\\lambda_i + \\mu) \\cdot v_i\n",
        "$$\n",
        "\n",
        "Entonces $v_i$ es autovector de $M + \\mu I$, con autovalor $\\gamma_i = \\lambda_i + \\mu$.\n",
        "\n",
        "\n",
        "Una matriz es invertible si todos sus autovalores son distintos de cero. Como:\n",
        "\n",
        "$$\n",
        "\\gamma_i = \\lambda_i + \\mu\n",
        "$$\n",
        "\n",
        "Si $\\lambda_i + \\mu \\ne 0$ para todo $i$, entonces $M + \\mu I$ es invertible.\n",
        "\n",
        "\n",
        "### b)\n",
        "\n",
        "Sea $L = K - A$ sus entradas son:\n",
        "- $L_{ii} = k_i$\n",
        "- $L_{ij} = -1$ si existe arista entre $i$ y $j$ ($i \\ne j$)\n",
        "- $L_{ij} = 0$ si no hay arista entre $i$ y $j$\n",
        "\n",
        "Para cada fila $i$, la suma de los valores absolutos fuera de la diagonal es:\n",
        "$$\n",
        "\\sum_{j \\ne i} |L_{ij}| = \\sum_{j \\ne i} A_{ij} = k_i = L_{ii}\n",
        "$$\n",
        "\n",
        "Entonces, $L$ es diagonalmente dominante, pero no estrictamente:\n",
        "$$\n",
        "|L_{ii}| = \\sum_{j \\ne i} |L_{ij}|\n",
        "$$\n",
        "\n",
        "Ahora, consideremos la matriz $L + \\mu I$, con $\\mu > 0$. Sus entradas son:\n",
        "$$\n",
        "(L + \\mu I)_{ii} = L_{ii} + (\\mu I)_{ii} = L_{ii} + \\mu = k_i + \\mu\n",
        "$$\n",
        "$$\n",
        "(L + \\mu I)_{ij} = L_{ij} + (\\mu I)_{ij} = L_{ij}\n",
        "$$\n",
        "\n",
        "Entonces, para cada fila $i$:\n",
        "$$\n",
        "|(L + \\mu I)_{ii}| = k_i + \\mu > \\sum_{j \\ne i} |L_{ij}| = k_i = \\sum_{j \\ne i} |(L + \\mu I)_{ij}|\n",
        "$$\n",
        "\n",
        "Por lo tanto, $L + \\mu I$ es estrictamente diagonal dominante.\n",
        "\n",
        "Por la proposición 4.2 de la página 83 del apunte de Acosta y Laplagne entonces $$L + \\mu I \\text{ es invertible}$$\n",
        "\n",
        "El método de la potencia aplicado a una matriz $M$ converge al autovector dominante, es decir, al autovector asociado al autovalor de mayor módulo.\n",
        "\n",
        "Los autovalores de $L$ son $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$, con $\\lambda_1 = 0$ como vimos en la presentación del tp.\n",
        "\n",
        "Supongamos que $L v = \\lambda v$, con $v \\ne 0$. Entonces:\n",
        "\n",
        "$$\n",
        "(L + \\mu I) v = L v + \\mu I v = \\lambda v + \\mu v = (\\lambda + \\mu) v\n",
        "$$\n",
        "\n",
        "Esto demuestra que $v$ sigue siendo un autovector de $L + \\mu I$, y que el nuevo autovalor asociado es $\\lambda + \\mu$.\n",
        "\n",
        "En consecuencia, $L + \\mu I$ tiene autovalores:\n",
        "\n",
        "$$\n",
        "\\lambda_1 + \\mu,\\ \\lambda_2 + \\mu,\\ \\dots,\\ \\lambda_n + \\mu\n",
        "$$\n",
        "\n",
        "$$\n",
        "(L + \\mu I) v = (\\lambda + \\mu) v\n",
        "$$\n",
        "\n",
        "Multiplicamos ambos lados por $ (L + \\mu I)^{-1} $:\n",
        "\n",
        "$$\n",
        "(L + \\mu I)^{-1}(L + \\mu I) v = (L + \\mu I)^{-1}(\\lambda + \\mu) v\n",
        "$$\n",
        "\n",
        "$$\n",
        "v = (L + \\mu I)^{-1}(\\lambda + \\mu) v\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\lambda + \\mu \\ne 0 \\text{ porque } \\mu > 0 \\text{ y } \\lambda \\geq 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "(L + \\mu I)^{-1} v = \\frac{1}{\\lambda + \\mu} v\n",
        "$$\n",
        "\n",
        "Esto muestra que cada autovector $v_i$ de $L + \\mu I$ es también autovector de su inversa, y el autovalor asociado es:\n",
        "\n",
        "$$\n",
        "\\frac{1}{\\lambda_i + \\mu}\n",
        "$$\n",
        "\n",
        "Por lo tanto, los autovalores de $ (L + \\mu I)^{-1} $ son:\n",
        "\n",
        "$$\n",
        "\\frac{1}{\\lambda_1 + \\mu},\\ \\frac{1}{\\lambda_2 + \\mu},\\ \\dots,\\ \\frac{1}{\\lambda_n + \\mu}\n",
        "$$\n",
        "\n",
        "Y como $\\lambda_1 = 0$, el mayor valor de $\\frac{1}{\\lambda_i + \\mu}$ ocurre cuando $\\lambda_i = 0$, o sea:\n",
        "\n",
        "$$\n",
        "\\max \\left( \\frac{1}{\\lambda_i + \\mu} \\right) = \\frac{1}{\\mu}\n",
        "$$\n",
        "\n",
        "El método de la potencia aplicado a $ (L + \\mu I)^{-1} $ converge al autovector asociado al autovalor más chico de $L + \\mu I$, que es:\n",
        "\n",
        "$$\n",
        "\\lambda_{\\min} = \\mu\n",
        "$$\n",
        "\n",
        "Ya sabemos que $L \\mathbf{1} = 0 \\cdot \\mathbf{1}$, por lo tanto:\n",
        "\n",
        "$$\n",
        "(L + \\mu I) \\mathbf{1} = L  \\mathbf{1} + \\mu \\cdot \\mathbf{1} = \\mu \\cdot \\mathbf{1}\n",
        "$$\n",
        "\n",
        "Y entonces:\n",
        "\n",
        "$$\n",
        "(L + \\mu I)^{-1} \\mathbf{1} = \\frac{1}{\\mu} \\cdot \\mathbf{1}\n",
        "$$\n",
        "\n",
        "El vector de unos $\\mathbf{1}$ es el autovector correspondiente al autovalor $\\frac{1}{\\mu}$ de la matriz inversa.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### c)\n",
        "\n",
        "$M$ es una matriz simétrica con autovectores ortogonales $v_1, v_2, \\dots, v_n$, y $v_1$ es el autovector asociado al autovalor $\\lambda_1$, es decir:\n",
        "\n",
        "$$\n",
        "M v_1 = \\lambda_1 v_1\n",
        "$$\n",
        "\n",
        "Definimos la matriz deflacionada:\n",
        "\n",
        "$$\n",
        "\\tilde{M} = M - \\frac{\\lambda_1}{v_1^T v_1} v_1 v_1^T\n",
        "$$\n",
        "\n",
        "Aplicamos $\\tilde{M}$ al autovector $v_1$:\n",
        "\n",
        "$$\n",
        "\\tilde{M} v_1 = M v_1 - \\frac{\\lambda_1}{v_1^T v_1} v_1 v_1^T v_1\n",
        "$$\n",
        "\n",
        "Como $M v_1 = \\lambda_1 v_1$ y $v_1^T v_1 = \\|v_1\\|^2$, se obtiene:\n",
        "\n",
        "$$\n",
        "\\tilde{M} v_1 = \\lambda_1 v_1 - \\frac{\\lambda_1}{v_1^T v_1} v_1 (v_1^T v_1) = \\lambda_1 v_1 - \\lambda_1 v_1 = 0\n",
        "$$\n",
        "\n",
        "Por lo tanto, $v_1$ es un autovector de $\\tilde{M}$ con autovalor 0.\n",
        "\n",
        "Ahora aplicamos $\\tilde{M}$ a un autovector $v_j$ ortogonal a $v_1$.\n",
        "\n",
        "Como $v_1^T v_j = 0$, se tiene:\n",
        "\n",
        "$$\n",
        "\\tilde{M} v_j = M v_j - \\frac{\\lambda_1}{v_1^T v_1} v_1 (v_1^T v_j) = M v_j - 0 = M v_j = \\lambda_j v_j\n",
        "$$\n",
        "\n",
        "Es decir, $v_j$ sigue siendo autovector de $\\tilde{M}$ con el mismo autovalor que en $M$.\n",
        "\n"
      ],
      "metadata": {
        "id": "t6l2IbJqzIcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 3\n"
      ],
      "metadata": {
        "id": "KBIB1X7R0f2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de paquetes necesarios para graficar\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Para leer archivos\n",
        "import geopandas as gpd # Para hacer cosas geográficas\n",
        "import seaborn as sns # Para hacer plots lindos\n",
        "import networkx as nx # Construcción de la red en NetworkX\n",
        "import scipy\n",
        "\n",
        "# Leemos el archivo, retenemos aquellos museos que están en CABA, y descartamos aquellos que no tienen latitud y longitud\n",
        "museos = gpd.read_file('https://raw.githubusercontent.com/MuseosAbiertos/Leaflet-museums-OpenStreetMap/refs/heads/principal/data/export.geojson')\n",
        "barrios = gpd.read_file('https://cdn.buenosaires.gob.ar/datosabiertos/datasets/ministerio-de-educacion/barrios/barrios.geojson')\n",
        "\n",
        "# En esta línea:\n",
        "# Tomamos museos, lo convertimos al sistema de coordenadas de interés, extraemos su geometría (los puntos del mapa),\n",
        "# calculamos sus distancias a los otros puntos de df, redondeamos (obteniendo distancia en metros), y lo convertimos a un array 2D de numpy\n",
        "D = museos.to_crs(\"EPSG:22184\").geometry.apply(lambda g: museos.to_crs(\"EPSG:22184\").distance(g)).round().to_numpy()\n",
        "\n",
        "def construye_adyacencia(D,m):\n",
        "    # Función que construye la matriz de adyacencia del grafo de museos\n",
        "    # D matriz de distancias, m cantidad de links por nodo\n",
        "    # Retorna la matriz de adyacencia como un numpy.\n",
        "    D = D.copy()\n",
        "    l = [] # Lista para guardar las filas\n",
        "    for fila in D: # recorriendo las filas, anexamos vectores lógicos\n",
        "        l.append(fila<=fila[np.argsort(fila)[m]] ) # En realidad, elegimos todos los nodos que estén a una distancia menor o igual a la del m-esimo más cercano\n",
        "    A = np.asarray(l).astype(int) # Convertimos a entero\n",
        "    np.fill_diagonal(A,0) # Borramos diagonal para eliminar autolinks\n",
        "    return(A)\n",
        "\n",
        "m = 3 # Cantidad de links por nodo\n",
        "A = construye_adyacencia(D,m)"
      ],
      "metadata": {
        "id": "G1BEwYyWxE6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7xwQB-TTxOAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a)"
      ],
      "metadata": {
        "id": "o9Byyxn67H4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "# i)\n",
        "\n",
        "def calcula_L(A):\n",
        "    \"\"\"\n",
        "    Calcula la matriz Laplaciana L = K - A.\n",
        "    Parámetros:\n",
        "        A (ndarray): matriz de adyacencia\n",
        "    Retorna:\n",
        "        L (ndarray): matriz Laplaciana\n",
        "    \"\"\"\n",
        "    grados = np.sum(A, axis=1)\n",
        "    K = np.diag(grados)\n",
        "    L = K - A\n",
        "    return L\n",
        "\n",
        "def calcula_R(A):\n",
        "    \"\"\"\n",
        "    Calcula la matriz de modularidad R = A - P con Pij = ki*kj / 2E\n",
        "    Parámetros:\n",
        "        A (ndarray): matriz de adyacencia\n",
        "    Retorna:\n",
        "        R (ndarray): matriz de modularidad\n",
        "    \"\"\"\n",
        "    grados = np.sum(A, axis=1)\n",
        "    E = np.sum(grados) / 2\n",
        "    P = np.outer(grados, grados) / (2 * E)\n",
        "    R = A - P\n",
        "    return R\n",
        "\n",
        "# ii)\n",
        "\n",
        "def calcula_lambda(L, v):\n",
        "    \"\"\"\n",
        "    Calcula el corte Lambda = (1/4) * s^T L s, donde s_i = signo(v_i).\n",
        "\n",
        "    Parameters:\n",
        "        L (numpy.ndarray): Matriz Laplaciana (n x n).\n",
        "        v (numpy.ndarray): Vector (n x 1) para calcular s.\n",
        "\n",
        "    Returns:\n",
        "        lamb (float): Valor del corte Lambda.\n",
        "    \"\"\"\n",
        "    # Calculamos s como el signo de los elementos de v\n",
        "    s = np.sign(v)\n",
        "    # Calculamos Lambda = (1/4) * s^T L s\n",
        "    lamb = (1/4) * s.T @ L @ s\n",
        "    return lamb\n",
        "\n",
        "def calcula_Q(R, v):\n",
        "    \"\"\"\n",
        "    Calcula la modularidad Q = (1/(4*E)) * s^T R s, donde s_i = signo(v_i).\n",
        "\n",
        "    Parameters:\n",
        "        R (numpy.ndarray): Matriz de modularidad (n x n).\n",
        "        v (numpy.ndarray): Vector (n x 1) para calcular s.\n",
        "\n",
        "    Returns:\n",
        "        Q (float): Valor de la modularidad (a menos del factor 1/(2*E)).\n",
        "    \"\"\"\n",
        "    # Calculamos s como el signo de los elementos de v\n",
        "    s = np.sign(v)\n",
        "    # Calculamos Q = s^T R s (sin el factor 1/(4*E))\n",
        "    Q = s.T @ R @ s\n",
        "    return Q\n",
        "\n",
        "\n",
        "A_ejemplo = np.array([\n",
        "      [0, 1, 1, 1, 0, 0, 0, 0],\n",
        "      [1, 0, 1, 1, 0, 0, 0, 0],\n",
        "      [1, 1, 0, 1, 0, 1, 0, 0],\n",
        "      [1, 1, 1, 0, 1, 0, 0, 0],\n",
        "      [0, 0, 0, 1, 0, 1, 1, 1],\n",
        "      [0, 0, 1, 0, 1, 0, 1, 1],\n",
        "      [0, 0, 0, 0, 1, 1, 0, 1],\n",
        "      [0, 0, 0, 0, 1, 1, 1, 0],\n",
        "])\n",
        "\n",
        "# Construimos L y R\n",
        "L_ej = calcula_L(A_ejemplo)\n",
        "R_ej = calcula_R(A_ejemplo)\n",
        "\n",
        "# Autovector esperado para el corte intuitivo s = (1,1,1,1,-1,-1,-1,-1)\n",
        "s_esperado = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n",
        "\n",
        "Lambda = (s_esperado.T @ L_ej @ s_esperado) / 4.0\n",
        "Q_tilde = s_esperado.T @ R_ej @ s_esperado\n",
        "\n",
        "print(f\"Λ esperado = {Lambda:.1f}\")\n",
        "print(f\"sᵀ R s    = {Q_tilde:.1f}\")\n"
      ],
      "metadata": {
        "id": "n9Kk9mpmkiyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490f55c9-f267-414c-de98-6e206d203064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Λ esperado = 2.0\n",
            "sᵀ R s    = 20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b)\n",
        "\n"
      ],
      "metadata": {
        "id": "_OCMUHSN7Cdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i)\n",
        "def metpot1(M, tol=1e-8, maxrep=np.inf):\n",
        "    \"\"\"\n",
        "    Aplica el método de la potencia para encontrar el autovalor de mayor módulo y su autovector.\n",
        "\n",
        "    Parameters:\n",
        "        M (numpy.ndarray): Matriz simétrica (n x n).\n",
        "        tol (float): Tolerancia para la convergencia.\n",
        "        maxrep (float): Número máximo de iteraciones.\n",
        "\n",
        "    Returns:\n",
        "        v1 (numpy.ndarray): Autovector asociado al autovalor de mayor módulo.\n",
        "        l (float): Autovalor de mayor módulo.\n",
        "        converged (bool): Indica si el método convergió.\n",
        "    \"\"\"\n",
        "    n = M.shape[0]\n",
        "    # Generamos un vector de partida aleatorio\n",
        "    v = np.random.uniform(-1, 1, n)\n",
        "    # Normalizamos el vector\n",
        "    v = v / np.linalg.norm(v)\n",
        "    # Aplicamos la matriz\n",
        "    v1 = M @ v\n",
        "    # Normalizamos\n",
        "    v1 = v1 / np.linalg.norm(v1)\n",
        "    # Calculamos el autovalor estimado\n",
        "    l = v.T @ M @ v\n",
        "    l1 = v1.T @ M @ v1\n",
        "    nrep = 0\n",
        "    # Iteramos hasta convergencia o alcanzar maxrep\n",
        "    while np.abs(l1 - l) / np.abs(l) > tol and nrep < maxrep:\n",
        "        v = v1\n",
        "        l = l1\n",
        "        v1 = M @ v\n",
        "        v1 = v1 / np.linalg.norm(v1)\n",
        "        l1 = v1.T @ M @ v1\n",
        "        nrep += 1\n",
        "    if not nrep < maxrep:\n",
        "        print('MaxRep alcanzado')\n",
        "    # Calculamos el autovalor final\n",
        "    l = v1.T @ M @ v1\n",
        "    return v1, l, nrep < maxrep\n",
        "\n",
        "# ii)\n",
        "\n",
        "def deflaciona(M, tol=1e-8, maxrep=np.inf):\n",
        "    \"\"\"\n",
        "    Calcula la matriz deflacionada eliminando el autovalor principal.\n",
        "\n",
        "    Parameters:\n",
        "        M (numpy.ndarray): Matriz simétrica (n x n).\n",
        "        tol (float): Tolerancia para el método de la potencia.\n",
        "        maxrep (float): Número máximo de iteraciones.\n",
        "\n",
        "    Returns:\n",
        "        deflA (numpy.ndarray): Matriz deflacionada.\n",
        "    \"\"\"\n",
        "    # Calculamos el autovector y autovalor principal\n",
        "    v1, l1, _ = metpot1(M, tol, maxrep)\n",
        "    # Calculamos la matriz deflacionada\n",
        "    deflA = M - l1 * np.outer(v1, v1) / np.dot(v1, v1)\n",
        "    return deflA\n",
        "\n",
        "# iii)\n",
        "\n",
        "def calculaLU(matriz):\n",
        "  # matriz es una matriz de NxN\n",
        "  # Retorna la factorización LU a través de una lista con dos matrices L y U de NxN.\n",
        "  matriz = matriz.copy()\n",
        "  n = matriz.shape[0]\n",
        "  L = np.eye(n)\n",
        "  U = matriz.astype(float)\n",
        "  for j in range(n):\n",
        "      for i in range(j+1, n):\n",
        "          L[i, j] = U[i, j] / U[j, j]\n",
        "          U[i, j:] -= L[i, j] * U[j, j:]\n",
        "\n",
        "  return [L, U]\n",
        "\n",
        "def metpotI(A, mu, tol=1e-8, maxrep=np.inf):\n",
        "    \"\"\"\n",
        "    Aplica el método de la potencia inversa para encontrar el autovalor más chico\n",
        "    de A + mu*I y su autovector.\n",
        "\n",
        "    Parameters:\n",
        "        A (numpy.ndarray): Matriz simétrica (n x n).\n",
        "        mu (float): Valor de desplazamiento.\n",
        "        tol (float): Tolerancia para la convergencia.\n",
        "        maxrep (float): Número máximo de iteraciones.\n",
        "\n",
        "    Returns:\n",
        "        v (numpy.ndarray): Autovector asociado al autovalor más chico de A + mu*I.\n",
        "        l (float): Autovalor correspondiente de A.\n",
        "        converged (bool): Indica si el método convergió.\n",
        "    \"\"\"\n",
        "    # Calcula A + mu*I\n",
        "    n = A.shape[0]\n",
        "    A_shifted = A + mu * np.eye(n)\n",
        "    # Factoriza A + mu*I usando LU\n",
        "    L, U = calculaLU(A_shifted)\n",
        "    # Resuelve el sistema lineal en cada iteración\n",
        "    v, l_inv, converged = metpot1(np.linalg.inv(A_shifted), tol, maxrep)\n",
        "    # Calcula el autovalor original: l = 1/l_inv - mu\n",
        "    l = 1 / l_inv - mu if l_inv != 0 else 0\n",
        "    return v, l, converged\n",
        "\n",
        "# iv)\n",
        "\n",
        "def metpot2(A, v1, l1, tol=1e-8, maxrep=np.inf):\n",
        "    \"\"\"\n",
        "    Aplica el método de la potencia para encontrar el segundo autovalor de A,\n",
        "    suponiendo que los autovectores son ortogonales.\n",
        "\n",
        "    Parameters:\n",
        "        A (numpy.ndarray): Matriz simétrica (n x n).\n",
        "        v1 (numpy.ndarray): Autovector asociado al autovalor principal.\n",
        "        l1 (float): Autovalor principal.\n",
        "        tol (float): Tolerancia para la convergencia.\n",
        "        maxrep (float): Número máximo de iteraciones.\n",
        "\n",
        "    Returns:\n",
        "        v2 (numpy.ndarray): Autovector asociado al segundo autovalor.\n",
        "        l2 (float): Segundo autovalor.\n",
        "        converged (bool): Indica si el método convergió.\n",
        "    \"\"\"\n",
        "    # Calcula la matriz deflacionada\n",
        "    deflA = A - l1 * np.outer(v1, v1) / np.dot(v1, v1)\n",
        "    # Aplica el método de la potencia a la matriz deflacionada\n",
        "    v2, l2, converged = metpot1(deflA, tol, maxrep)\n",
        "    return v2, l2, converged\n",
        "\n",
        "def metpotI2(A, mu, tol=1e-8, maxrep=np.inf):\n",
        "    \"\"\"\n",
        "    Calcula el segundo autovalor más chico de A + mu*I, suponiendo que A tiene\n",
        "    un autovalor igual a 0 y los demás son positivos.\n",
        "\n",
        "    Parameters:\n",
        "        A (numpy.ndarray): Matriz simétrica (n x n).\n",
        "        mu (float): Valor de desplazamiento.\n",
        "        tol (float): Tolerancia para la convergencia.\n",
        "        maxrep (float): Número máximo de iteraciones.\n",
        "\n",
        "    Returns:\n",
        "        v (numpy.ndarray): Autovector asociado al segundo autovalor más chico.\n",
        "        l (float): Segundo autovalor más chico de A.\n",
        "        converged (bool): Indica si el método convergió.\n",
        "    \"\"\"\n",
        "    # Calcula A + mu*I\n",
        "    n = A.shape[0]\n",
        "    X = A + mu * np.eye(n)\n",
        "    # Invierte X\n",
        "    iX = np.linalg.inv(X)\n",
        "    # Encuentra el autovector y autovalor principal de iX\n",
        "    v1, l1, _ = metpot1(iX, tol, maxrep)\n",
        "    # Aplica metpot2 para encontrar el segundo autovalor de iX\n",
        "    v, l_inv, converged = metpot2(iX, v1, l1, tol, maxrep)\n",
        "    # Calcula el autovalor original: l = 1/l_inv - mu\n",
        "    l = 1 / l_inv - mu if l_inv != 0 else 0\n",
        "    return v, l, converged\n",
        "\n",
        "\n",
        "\n",
        "alpha = 1/5\n",
        "mu = 2\n",
        "metpotI_ejemplo = metpotI(A_ejemplo, mu)\n",
        "metpotI2_ejemplo = metpotI2(A_ejemplo, mu)\n",
        "\n",
        "# print(f\"Autovalor más chico = {metpotI_ejemplo[1]:.1f}\")\n",
        "# print(f\"Autovector más chico de L = {metpotI_ejemplo[0]}\")\n",
        "# print(f\"Segundo autovalor más chico = {metpotI2_ejemplo[1]:.1f}\")\n",
        "# print(f\"Autovector más chico de L = {metpotI2_ejemplo[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "L_ej = calcula_L(A_ejemplo)\n",
        "R_ej = calcula_R(A_ejemplo)\n",
        "\n",
        "# Autovalor principal y segundo de L usando método de la potencia y deflación\n",
        "v1_L, l1_L, _ = metpot1(L_ej)\n",
        "v2_L, l2_L, _ = metpot2(L_ej, v1_L, l1_L)\n",
        "s_L = np.sign(v2_L)\n",
        "s_L[s_L == 0] = 1\n",
        "\n",
        "# Autovalor principal de R usando método de la potencia\n",
        "v1_R, l1_R, _ = metpot1(R_ej)\n",
        "s_R = np.sign(v1_R)\n",
        "s_R[s_R == 0] = 1\n",
        "\n",
        "# Cálculo de métricas\n",
        "Lambda = calcula_lambda(L_ej, v2_L)\n",
        "Q_tilde = calcula_Q(R_ej, v1_R)\n",
        "\n",
        "print(\"--- Segundo autovector de L ---\")\n",
        "print(f\"Λ = {Lambda:.2f}\")\n",
        "print(f\"s (L) = {s_L}\")\n",
        "print(\"\\n--- Principal autovector de R ---\")\n",
        "print(f\"sᵀ R s = {Q_tilde:.2f}\")\n",
        "print(f\"s (R) = {s_R}\")\n",
        "\n",
        "s_esperado = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n",
        "print(\"\\n--- Partición esperada ---\")\n",
        "print(f\"s esperado = {s_esperado}\")\n",
        "\n",
        "# No se si esta bien este punto, lo tengo que revisar\n"
      ],
      "metadata": {
        "id": "Y_gw8_aI7_Fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a215a892-b8c1-4dae-cff1-c25ac5de0ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Segundo autovector de L ---\n",
            "Λ = 10.00\n",
            "s (L) = [-1. -1.  1.  1. -1. -1.  1.  1.]\n",
            "\n",
            "--- Principal autovector de R ---\n",
            "sᵀ R s = 20.00\n",
            "s (R) = [-1. -1. -1. -1.  1.  1.  1.  1.]\n",
            "\n",
            "--- Partición esperada ---\n",
            "s esperado = [ 1  1  1  1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c)"
      ],
      "metadata": {
        "id": "llfl5-2u7MXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i)\n",
        "\n",
        "def laplaciano_iterativo(A, niveles, nombres_s=None):\n",
        "    \"\"\"\n",
        "    Realiza particiones iterativas del grafo usando el segundo autovector del Laplaciano.\n",
        "\n",
        "    Parameters:\n",
        "        A (numpy.ndarray): Matriz de adyacencia simétrica (n x n).\n",
        "        niveles (int): Número de niveles de partición.\n",
        "        nombres_s (list): Lista de nombres de los nodos (opcional).\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de listas con los índices de los nodos en cada comunidad.\n",
        "    \"\"\"\n",
        "    if nombres_s is None:\n",
        "        nombres_s = range(A.shape[0])\n",
        "    if A.shape[0] == 1 or niveles == 0:\n",
        "        return [nombres_s]\n",
        "    else:\n",
        "        L = calcula_L(A)\n",
        "        # Encuentra el segundo autovector más chico de L\n",
        "        v, l, _ = metpotI2(L, mu=1.0, tol=1e-8, maxrep=np.inf)\n",
        "        # Divide los nodos según el signo de v\n",
        "        idx_pos = [i for i, vi in enumerate(v) if vi > 0]\n",
        "        idx_neg = [i for i, vi in enumerate(v) if vi < 0]\n",
        "        # Crea submatrices de adyacencia\n",
        "        Ap = A[np.ix_(idx_pos, idx_pos)]\n",
        "        Am = A[np.ix_(idx_neg, idx_neg)]\n",
        "        # Nombres para cada subgrafo\n",
        "        nombres_pos = [nombres_s[i] for i in idx_pos]\n",
        "        nombres_neg = [nombres_s[i] for i in idx_neg]\n",
        "        return (\n",
        "            laplaciano_iterativo(Ap, niveles-1, nombres_pos) +\n",
        "            laplaciano_iterativo(Am, niveles-1, nombres_neg)\n",
        "        )\n",
        "\n",
        "# ii)\n",
        "\n",
        "def modularidad_iterativo(A=None, R=None, nombres_s=None):\n",
        "    \"\"\"\n",
        "    Realiza particiones iterativas del grafo maximizando la modularidad.\n",
        "\n",
        "    Parameters:\n",
        "        A (numpy.ndarray): Matriz de adyacencia simétrica (n x n) (opcional).\n",
        "        R (numpy.ndarray): Matriz de modularidad (n x n) (opcional).\n",
        "        nombres_s (list): Lista de nombres de los nodos (opcional).\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de listas con los índices de los nodos en cada comunidad.\n",
        "    \"\"\"\n",
        "    if A is None and R is None:\n",
        "        print('Dame una matriz')\n",
        "        return np.nan\n",
        "    if R is None:\n",
        "        R = calcula_R(A)\n",
        "    if nombres_s is None:\n",
        "        nombres_s = range(R.shape[0])\n",
        "    if R.shape[0] == 1:\n",
        "        return [nombres_s]\n",
        "    else:\n",
        "        # Encuentra el autovector principal de R\n",
        "        v, l, _ = metpot1(R, tol=1e-8, maxrep=np.inf)\n",
        "        # Calcula la modularidad actual\n",
        "        Q0 = np.sum(R[v > 0, :][:, v > 0]) + np.sum(R[v < 0, :][:, v < 0])\n",
        "        if Q0 <= 0 or np.all(v > 0) or np.all(v < 0):\n",
        "            return [nombres_s]\n",
        "        else:\n",
        "            # Divide los nodos según el signo de v\n",
        "            idx_pos = [i for i, vi in enumerate(v) if vi > 0]\n",
        "            idx_neg = [i for i, vi in enumerate(v) if vi < 0]\n",
        "            # Crea submatrices de modularidad\n",
        "            Rp = R[np.ix_(idx_pos, idx_pos)]\n",
        "            Rm = R[np.ix_(idx_neg, idx_neg)]\n",
        "            # Encuentra los autovectores principales de Rp y Rm\n",
        "            vp, lp, _ = metpot1(Rp, tol=1e-8, maxrep=np.inf)\n",
        "            vm, lm, _ = metpot1(Rm, tol=1e-8, maxrep=np.inf)\n",
        "            # Calcula el cambio en modularidad\n",
        "            Q1 = 0\n",
        "            if not (np.all(vp > 0) or np.all(vp < 0)):\n",
        "                Q1 += np.sum(Rp[vp > 0, :][:, vp > 0]) + np.sum(Rp[vp < 0, :][:, vp < 0])\n",
        "            if not (np.all(vm > 0) or np.all(vm < 0)):\n",
        "                Q1 += np.sum(Rm[vm > 0, :][:, vm > 0]) + np.sum(Rm[vm < 0, :][:, vm < 0])\n",
        "            if Q0 >= Q1:\n",
        "                return [[nombres_s[i] for i in idx_pos], [nombres_s[i] for i in idx_neg]]\n",
        "            else:\n",
        "                # Nombres para cada subgrafo\n",
        "                nombres_pos = [nombres_s[i] for i in idx_pos]\n",
        "                nombres_neg = [nombres_s[i] for i in idx_neg]\n",
        "                return (\n",
        "                    modularidad_iterativo(R=Rp, nombres_s=nombres_pos) +\n",
        "                    modularidad_iterativo(R=Rm, nombres_s=nombres_neg)\n",
        "                )\n",
        "\n",
        "comunidades_laplaciano = laplaciano_iterativo(A_ejemplo, niveles=2)\n",
        "comunidades_modularidad = modularidad_iterativo(A_ejemplo)\n",
        "\n",
        "print(\"\\n--- Partición óptima en 4 grupos ---\")\n",
        "print(\"\\nPor Laplaciano:\")\n",
        "print(comunidades_laplaciano)\n",
        "\n",
        "print(\"\\nPor Modularidad:\")\n",
        "print(comunidades_modularidad)"
      ],
      "metadata": {
        "id": "CE7s1q0i8DOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be22b3f-d64a-4bcd-fbdf-734bd175b906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Partición óptima en 4 grupos ---\n",
            "\n",
            "Por Laplaciano:\n",
            "[[5, 7], [4, 6], [0, 3], [1, 2]]\n",
            "\n",
            "Por Modularidad:\n",
            "[[0, 1, 2, 3], [4, 5, 6, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 4"
      ],
      "metadata": {
        "id": "qaoP1VFP1HIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punto 5\n"
      ],
      "metadata": {
        "id": "nwijISOU1JvQ"
      }
    }
  ]
}